def NODE = "rhcos-jenkins"
def AWS_REGION = "us-east-1"
def API_CI_REGISTRY = "registry.svc.ci.openshift.org"
def OS_NAME = "maipo";
def OSCONTAINER_IMG = API_CI_REGISTRY + "/rhcos/os-${OS_NAME}"

// location on the server we'll rsync to/from our $WORKSPACE
def images = "/srv/rhcos/output/images"

node(NODE) {
    stage("Clean workspace") {
       step([$class: 'WsCleanup'])
    }
    checkout scm
    utils = load("pipeline-utils.groovy")

    // This job should only be triggered by Jenkinsfile.cloud
    utils.define_properties(null)

    if (params.DRY_RUN) {
        echo "DRY_RUN set, skipping aws tests and launch permissions"
        currentBuild.result = 'NOT_BUILT'
        currentBuild.description = '(dry run)'
        return
    }

    // We're only ever triggered by the cloud job, so we know the latest build is in latest/
    // We immediately resolve it back to the specific images/ dir
    def ostree_commit, version
    try {
    utils.inside_assembler_container("") {
    stage("Sync In") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_file_in_dest(ARTIFACT_SERVER, KEY_FILE, "${images}/cloud/latest/meta.json", "${WORKSPACE}/meta.json")
            ostree_commit = utils.sh_capture("jq -r '.[\"ostree-commit\"]' ${WORKSPACE}/meta.json")
            version = utils.sh_capture("jq -r '.[\"ostree-version\"]' ${WORKSPACE}/meta.json")
            // resolve to original dir to avoid races in the next rsync in
            def imgv = utils.sh_capture("jq -r '.[\"image-version\"]' ${WORKSPACE}/meta.json")
            utils.rsync_file_in_dest(ARTIFACT_SERVER, KEY_FILE, "${images}/cloud/${imgv}/aws-${AWS_REGION}.json", "${WORKSPACE}/aws-${AWS_REGION}.json")
        }
    }

    // Number of parallel kola tests
    def NUM_VMS = "10"
    try {
    stage("Run Kola tests on intermediate aws image") {
            withCredentials([
                [$class: 'AmazonWebServicesCredentialsBinding', credentialsId: params.AWS_CREDENTIALS],
                string(credentialsId: params.S3_PRIVATE_BUCKET, variable: 'S3_PRIVATE_BUCKET'),
                string(credentialsId: params.AWS_CI_ACCOUNT, variable: 'AWS_CI_ACCOUNT'),
                string(credentialsId: params.S3_PUBLIC_BUCKET, variable: 'S3_PUBLIC_BUCKET'),
                usernameColonPassword(credentialsId: params.REGISTRY_CREDENTIALS, variable: 'CREDS'),
            ]) {
                def ami_intermediate = utils.sh_capture("jq -r .HVM ${WORKSPACE}/aws-${AWS_REGION}.json")

                // login to registry and setup container storage
                def (registryUser, registryPass) = "${CREDS}".split(':')
                utils.registry_login(registryUser, registryPass, "${API_CI_REGISTRY}")
                utils.prep_container_storage("${WORKSPACE}")

                currentBuild.description = "version=${version} ami=${ami_intermediate}"
                sh """
                    # Do testing with intermediate aws image passed in by cloud job
                    if ! kola -b rhcos -p aws --aws-type t2.small --tapfile rhcos-aws.tap --aws-ami ${ami_intermediate} --aws-region ${AWS_REGION} -j ${NUM_VMS} run; then
                        # if the tests fail, GC the container image tagged with the ostree commit
                        skopeo delete docker://${OSCONTAINER_IMG}:${ostree_commit}
                        exit 1
                    fi

                    # Tests pass, tag the json in the artifact server to a persistent location
                    # and give launch permissions to OpenShift CI
                    export AWS_DEFAULT_REGION=${AWS_REGION}
                    aws ec2 create-tags \
                        --resources ${ami_intermediate} \
                        --tags rhcos_tag=alpha
                    aws ec2 modify-image-attribute \
                        --image-id ${ami_intermediate} \
                        --launch-permission '{"Add":[{"Group":"all"}]}'

                    # Upload the json file to a public location
                    aws s3 cp --acl public-read \
                        ${WORKSPACE}/aws-${AWS_REGION}-tested.json \
                        s3://${S3_PUBLIC_BUCKET}/aws-${AWS_REGION}-tested.json

                    # Tag the container image to alpha, then GC the image tagged with the ostree commit
                    podman pull ${OSCONTAINER_IMG}:${ostree_commit}
                    podman tag ${OSCONTAINER_IMG}:${ostree_commit} ${OSCONTAINER_IMG}:alpha
                    podman push ${OSCONTAINER_IMG}:alpha
                    skopeo delete docker://${OSCONTAINER_IMG}:${ostree_commit}
                """
            }
        }
    } finally {
        sh 'if test -e _kola_temp; then tar -cJf _kola_temp.tar.xz _kola_temp; fi'
        archiveArtifacts artifacts: "_kola_temp.tar.xz", allowEmptyArchive: true
        archiveArtifacts artifacts: "rhcos-aws.tap", allowEmptyArchive: true
    }

    stage("rsync out") {
        withCredentials([
            string(credentialsId: params.ARTIFACT_SERVER, variable: 'ARTIFACT_SERVER'),
            sshUserPrivateKey(credentialsId: params.ARTIFACT_SSH_CREDS_ID, keyFileVariable: 'KEY_FILE'),
        ]) {
            utils.rsync_file_out_dest(ARTIFACT_SERVER, KEY_FILE, "${WORKSPACE}/aws-${AWS_REGION}.json", "${images}/aws-${AWS_REGION}-tested.json")
        }
    }
    }
    } catch (Throwable e) {
        currentBuild.result = 'FAILURE'
        throw e
    } finally {
        utils.notify_status_change currentBuild
    }
}
